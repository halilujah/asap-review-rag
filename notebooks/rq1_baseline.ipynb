{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12dd16a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: ICLR_2018_12\n",
      "Baseline Input:\n",
      " Title: Spectral Normalization for Generative Adversarial Networks\n",
      "Abstract: \n",
      "RAG Input Preview:\n",
      " Title: Spectral Normalization for Generative Adversarial Networks\n",
      "Abstract: \n",
      "\n",
      "Retrieved Reviews:\n",
      "This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives. The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator. This Lipschitz property has already been proposed by recent methods and has showed some success. However,  the authors here argue that spec\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Set paper ID\n",
    "paper_id = \"ICLR_2018_12\"\n",
    "\n",
    "# Define paths\n",
    "paper_path = f\"../data/dataset/ICLR_2018/ICLR_2018_paper/{paper_id}_paper.json\"\n",
    "review_path = f\"../data/dataset/ICLR_2018/ICLR_2018_review/{paper_id}_review.json\"\n",
    "\n",
    "# Load files\n",
    "with open(paper_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    paper = json.load(f)\n",
    "\n",
    "with open(review_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    review = json.load(f)\n",
    "\n",
    "# Extract content\n",
    "title = paper.get(\"title\", \"\")\n",
    "abstract = paper.get(\"abstract\", \"\")\n",
    "reviews = [r.get(\"review\", \"\") for r in review.get(\"reviews\", [])[:3]]\n",
    "\n",
    "# Prepare model inputs\n",
    "baseline_input = f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "rag_input = f\"{baseline_input}\\n\\nRetrieved Reviews:\\n\" + \"\\n\\n\".join(reviews)\n",
    "\n",
    "# Print preview\n",
    "print(\"Paper ID:\", paper.get(\"id\"))\n",
    "print(\"Baseline Input:\\n\", baseline_input[:300])\n",
    "print(\"RAG Input Preview:\\n\", rag_input[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c841e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Spectral Normalization for Generative Adversarial Networks\n",
      "Abstract: Â Some of the most popular kinds of adaptive networks are typically characterized by strong adaptive aspects, particularly about how to recognize one another and for each other, how to manipulate the state of the network through various strategies and processes (see my blog for more). In this article we describe an interesting approach that is highly selective, in that we are able to learn a model that can learn very fast, and where to look for\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer.encode(baseline_input, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=100, do_sample=True)\n",
    "\n",
    "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2e290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nRagRetriever requires the ðŸ¤— Datasets library but it was not found in your environment. You can install it with:\n```\npip install datasets\n```\nIn a notebook or a colab, you can install it by executing a cell with\n```\n!pip install datasets\n```\nthen restarting your kernel.\n\nNote that if you have a local folder named `datasets` or a local python file named `datasets.py` in your current\nworking directory, python may try to import this instead of the ðŸ¤— Datasets library. You should rename this folder or\nthat python file if that's the case. Please note that you may need to restart your runtime after installation.\n\nRagRetriever requires the faiss library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RagTokenizer, RagTokenForGeneration, RagRetriever\n\u001b[32m      2\u001b[39m tokenizer = RagTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mfacebook/rag-token-nq\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m retriever = \u001b[43mRagRetriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/rag-token-nq\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexact\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_dummy_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m model = RagTokenForGeneration.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mfacebook/rag-token-nq\u001b[39m\u001b[33m\"\u001b[39m, retriever=retriever)\n\u001b[32m      6\u001b[39m input_text = title  \u001b[38;5;66;03m# Cut off long text manually for now\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:442\u001b[39m, in \u001b[36mRagRetriever.from_pretrained\u001b[39m\u001b[34m(cls, retriever_name_or_path, indexed_dataset, **kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, retriever_name_or_path, indexed_dataset=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfaiss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m     config = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m RagConfig.from_pretrained(retriever_name_or_path, **kwargs)\n\u001b[32m    444\u001b[39m     rag_tokenizer = RagTokenizer.from_pretrained(retriever_name_or_path, config=config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1828\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1826\u001b[39m failed = [msg.format(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[32m   1827\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1828\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nRagRetriever requires the ðŸ¤— Datasets library but it was not found in your environment. You can install it with:\n```\npip install datasets\n```\nIn a notebook or a colab, you can install it by executing a cell with\n```\n!pip install datasets\n```\nthen restarting your kernel.\n\nNote that if you have a local folder named `datasets` or a local python file named `datasets.py` in your current\nworking directory, python may try to import this instead of the ðŸ¤— Datasets library. You should rename this folder or\nthat python file if that's the case. Please note that you may need to restart your runtime after installation.\n\nRagRetriever requires the faiss library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "\n",
    "# Load tokenizer, model, and retriever\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "# Your input text (title only)\n",
    "input_text = title\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate summary\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
