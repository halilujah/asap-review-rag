{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552a139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_introduction_text(content_json):\n",
    "    \"\"\"\n",
    "    Safely extract the '1 INTRODUCTION' section from a paper content.json.\n",
    "    \"\"\"\n",
    "    # Try 'metadata' block first, fallback to root\n",
    "    sections = []\n",
    "\n",
    "    if \"metadata\" in content_json and isinstance(content_json[\"metadata\"], dict):\n",
    "        sections = content_json[\"metadata\"].get(\"sections\", [])\n",
    "    else:\n",
    "        sections = content_json.get(\"sections\", [])\n",
    "\n",
    "    for section in sections:\n",
    "        heading = section.get(\"heading\")\n",
    "        if heading:\n",
    "            heading_clean = heading.strip().upper()\n",
    "            if heading_clean.startswith(\"1 INTRODUCTION\"):\n",
    "                return section.get(\"text\", \"\")\n",
    "\n",
    "    return \"\"  # return empty string if not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40dcae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "base_dir = \"../data/dataset\"\n",
    "conferences = [\n",
    "    \"ICLR_2017\", \"ICLR_2018\", \"ICLR_2019\", \"ICLR_2020\",\n",
    "    \"NIPS_2016\", \"NIPS_2017\", \"NIPS_2018\", \"NIPS_2019\"\n",
    "]\n",
    "\n",
    "titles = []\n",
    "paper_ids = []\n",
    "title_intro_texts = []\n",
    "paper_decisions = {}\n",
    "review_scores = {}\n",
    "\n",
    "for conf in conferences:\n",
    "    paper_dir = os.path.join(base_dir, f\"{conf}/{conf}_paper\")\n",
    "    content_dir = os.path.join(base_dir, f\"{conf}/{conf}_content\")\n",
    "\n",
    "    for fname in os.listdir(paper_dir):\n",
    "        if fname.endswith(\".json\"):\n",
    "            with open(os.path.join(paper_dir, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "                paper = json.load(f)\n",
    "                pid = paper.get(\"id\")\n",
    "                title = paper.get(\"title\", \"\")\n",
    "                decision = paper.get(\"decision\", \"Unknown\")\n",
    "\n",
    "                # Load introduction section\n",
    "                intro_text = \"\"\n",
    "                content_path = os.path.join(content_dir, f\"{pid}.json\")\n",
    "                if os.path.exists(content_path):\n",
    "                    with open(content_path, \"r\", encoding=\"utf-8\") as cfile:\n",
    "                        content = json.load(cfile)\n",
    "                        intro_text = extract_introduction_text(content)\n",
    "\n",
    "                if pid and title:\n",
    "                    titles.append(title)\n",
    "                    paper_ids.append(pid)\n",
    "                    paper_decisions[pid] = decision\n",
    "                    title_intro_texts.append(f\"Title: {title}\\n\\nIntroduction: {intro_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d29e1d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# Load Sentence-BERT model\n",
    "model1 = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model2 = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "model3 = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80bbbd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new embeddings\n",
    "title_intro_embeddings_np1 = np.array(model1.encode(title_intro_texts, convert_to_tensor=False), dtype='float32')\n",
    "title_intro_embeddings_np2 = np.array(model2.encode(title_intro_texts, convert_to_tensor=False), dtype='float32')\n",
    "title_intro_embeddings_np3 = np.array(model3.encode(title_intro_texts, convert_to_tensor=False), dtype='float32')\n",
    "\n",
    "# Create new FAISS index\n",
    "index1 = faiss.IndexFlatL2(title_intro_embeddings_np1.shape[1])\n",
    "index1.add(title_intro_embeddings_np1)\n",
    "\n",
    "index2 = faiss.IndexFlatL2(title_intro_embeddings_np2.shape[1])\n",
    "index2.add(title_intro_embeddings_np2)\n",
    "\n",
    "index3 = faiss.IndexFlatL2(title_intro_embeddings_np3.shape[1])\n",
    "index3.add(title_intro_embeddings_np3)\n",
    "\n",
    "models =[model1,model2,model3]\n",
    "indexes = [index1,index2,index3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b8060d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_scores = {}\n",
    "\n",
    "for conf in conferences:\n",
    "    review_dir = os.path.join(base_dir, f\"{conf}/{conf}_review\")\n",
    "\n",
    "    for fname in os.listdir(review_dir):\n",
    "        if fname.endswith(\".json\"):\n",
    "            with open(os.path.join(review_dir, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                pid = data.get(\"id\")\n",
    "                reviews = data.get(\"reviews\", [])\n",
    "\n",
    "                ratings, confidences = [], []\n",
    "\n",
    "                for review in reviews:\n",
    "                    try:\n",
    "                        ratings.append(int(review.get(\"rating\", \"\").split(\":\")[0]))\n",
    "                        confidences.append(int(review.get(\"confidence\", \"\").split(\":\")[0]))\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                if ratings:\n",
    "                    review_scores[pid] = {\n",
    "                        \"avg_rating\": round(sum(ratings) / len(ratings), 2),\n",
    "                        \"avg_confidence\": round(sum(confidences) / len(confidences), 2) if confidences else None,\n",
    "                        \"review_count\": len(ratings)\n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "132ea8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_decision(raw_decision):\n",
    "    \"\"\"\n",
    "    Normalize decision string into one of: 'Accept', 'Reject', or 'Other'.\n",
    "    - Accept if it starts with 'Accept'\n",
    "    - Reject if it starts with 'Reject'\n",
    "    - Otherwise, Other\n",
    "    \"\"\"\n",
    "    if not raw_decision:\n",
    "        return \"Other\"\n",
    "\n",
    "    lower_decision = raw_decision.strip().lower()\n",
    "    if lower_decision.startswith(\"accept\"):\n",
    "        return \"Accept\"\n",
    "    elif lower_decision.startswith(\"reject\"):\n",
    "        return \"Reject\"\n",
    "    else:\n",
    "        return \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9745a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_weighted_decision_and_scores(query_text, k=10,modelInd=0):\n",
    "    query_vec = models[modelInd].encode(query_text, convert_to_tensor=False)\n",
    "    query_vec_np = np.array([query_vec], dtype='float32')\n",
    "    D, I = indexes[modelInd].search(query_vec_np, k + 1)\n",
    "    top_k_indices = I[0][1:]\n",
    "    distances = D[0][1:]\n",
    "\n",
    "    # Compute weights (softmax over negative distance)\n",
    "    similarities = -distances\n",
    "    weights = np.exp(similarities) / np.sum(np.exp(similarities))\n",
    "\n",
    "    decision_weights = {}\n",
    "    weighted_rating = 0\n",
    "    weighted_conf = 0\n",
    "    weight_total_rating = 0\n",
    "    weight_total_conf = 0\n",
    "\n",
    "    for i, idx in enumerate(top_k_indices):\n",
    "        pid = paper_ids[idx]\n",
    "        decision = normalize_decision(paper_decisions.get(pid, \"Unknown\"))\n",
    "        weight = weights[i]\n",
    "\n",
    "        # Weight decision\n",
    "        decision_weights[decision] = decision_weights.get(decision, 0) + weight\n",
    "\n",
    "        # Weight scores\n",
    "        review = review_scores.get(pid, {})\n",
    "        rating = review.get(\"avg_rating\")\n",
    "        confidence = review.get(\"avg_confidence\")\n",
    "\n",
    "        if rating is not None:\n",
    "            weighted_rating += weight * rating\n",
    "            weight_total_rating += weight\n",
    "\n",
    "        if confidence is not None:\n",
    "            weighted_conf += weight * confidence\n",
    "            weight_total_conf += weight\n",
    "\n",
    "    # Pick decision with highest total weight\n",
    "    estimated_decision = max(decision_weights.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    est_rating = round(weighted_rating / weight_total_rating, 2) if weight_total_rating else None\n",
    "    est_conf = round(weighted_conf / weight_total_conf, 2) if weight_total_conf else None\n",
    "\n",
    "    return {\n",
    "        \"estimated_decision\": estimated_decision,\n",
    "        \"estimated_rating\": est_rating,\n",
    "        \"estimated_confidence\": est_conf,\n",
    "        \"decision_distribution\": decision_weights\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5cce39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def evaluate_accuracy_with_intro(n=100, k=10,modelInd=0):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confusion = Counter()\n",
    "\n",
    "    for i in range(min(n, len(titles))):\n",
    "        query_text = title_intro_texts[i]\n",
    "        query_pid = paper_ids[i]\n",
    "        true_decision = normalize_decision(paper_decisions.get(query_pid, \"Unknown\"))\n",
    "\n",
    "        result = estimate_weighted_decision_and_scores(query_text, k,modelInd)\n",
    "        pred_decision = result[\"estimated_decision\"]\n",
    "\n",
    "        if pred_decision == true_decision:\n",
    "            correct += 1\n",
    "        confusion[(true_decision, pred_decision)] += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    print(f\"\\nâœ… Initial Results Accuracy (title+intro, top-{k}): {accuracy:.2f}\")\n",
    "    print(\"\\nğŸ“Š Confusion Matrix (True â†’ Predicted):\")\n",
    "    for (true, pred), count in confusion.items():\n",
    "        print(f\"{true:25s} â†’ {pred:25s}: {count}\")\n",
    "\n",
    "    return accuracy, confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37a97da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single(i, k=10,modelInd=0):\n",
    "    query_text = title_intro_texts[i]\n",
    "    query_pid = paper_ids[i]\n",
    "    true_decision = paper_decisions.get(query_pid, \"Unknown\")\n",
    "    query_title = titles[i]\n",
    "\n",
    "    # Estimate using weighted method\n",
    "    result = estimate_weighted_decision_and_scores(query_text, k)\n",
    "    pred_decision = result[\"estimated_decision\"]\n",
    "    decision_weights = result[\"decision_distribution\"]\n",
    "\n",
    "    print(f\"\\nğŸ” Original Paper [{query_pid}]:\")\n",
    "    print(f\"Title: {query_title}\")\n",
    "    print(f\"True Decision: {true_decision}\")\n",
    "    print(f\"Predicted Decision: {pred_decision}\")\n",
    "    print(\"\\nğŸ“‹ Similar Papers and Their Decisions:\")\n",
    "\n",
    "    # Retrieve nearest neighbors and distances\n",
    "    query_vec = models[modelInd].encode(query_text, convert_to_tensor=False)\n",
    "    query_vec_np = np.array([query_vec], dtype='float32')\n",
    "    D, I = indexes[modelInd].search(query_vec_np, k + 1)\n",
    "    top_k_indices = I[0][1:]  # skip self\n",
    "    distances = D[0][1:]\n",
    "\n",
    "    similarities = -distances\n",
    "    weights = np.exp(similarities) / np.sum(np.exp(similarities))\n",
    "\n",
    "    for rank, (idx, weight) in enumerate(zip(top_k_indices, weights), start=1):\n",
    "        pid = paper_ids[idx]\n",
    "        title = titles[idx]\n",
    "        decision = paper_decisions.get(pid, \"Unknown\")\n",
    "        print(f\"{rank}. {title} [{pid}]\")\n",
    "        print(f\"   Decision: {decision} | Weight: {weight:.3f}\")\n",
    "\n",
    "    print(\"\\nğŸ“Š Weighted Decision Totals:\")\n",
    "    for label, w in sorted(decision_weights.items(), key=lambda x: -x[1]):\n",
    "        print(f\"{label}: {w:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5265f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0d34f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Original Paper [ICLR_2017_111]:\n",
      "Title: Trusting SVM for Piecewise Linear CNNs\n",
      "True Decision: Accept (Poster)\n",
      "Predicted Decision: Accept\n",
      "\n",
      "ğŸ“‹ Similar Papers and Their Decisions:\n",
      "1. A Unified View of Piecewise Linear Neural Network Verification [NIPS_2018_442]\n",
      "   Decision: Accept | Weight: 0.229\n",
      "2. Piecewise Linear Neural Networks verification: A comparative study [ICLR_2018_713]\n",
      "   Decision: Reject | Weight: 0.228\n",
      "3. Piecewise Strong Convexity of Neural Networks [NIPS_2019_1163]\n",
      "   Decision: Accept | Weight: 0.187\n",
      "4. SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability [NIPS_2017_582]\n",
      "   Decision: Accept | Weight: 0.178\n",
      "5. Cautious Deep Learning [ICLR_2019_1140]\n",
      "   Decision: Reject | Weight: 0.178\n",
      "\n",
      "ğŸ“Š Weighted Decision Totals:\n",
      "Accept: 0.594\n",
      "Reject: 0.406\n"
     ]
    }
   ],
   "source": [
    "evaluate_single(12,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-3): 0.55\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Accept                   : 55\n",
      "Accept                    â†’ Reject                   : 43\n",
      "Accept                    â†’ Other                    : 2\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-5): 0.57\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Accept                   : 57\n",
      "Accept                    â†’ Reject                   : 43\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-7): 0.56\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Accept                   : 56\n",
      "Accept                    â†’ Reject                   : 44\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-10): 0.58\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 42\n",
      "Accept                    â†’ Accept                   : 58\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-15): 0.58\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 42\n",
      "Accept                    â†’ Accept                   : 58\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-3): 0.56\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 44\n",
      "Accept                    â†’ Accept                   : 56\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-5): 0.59\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 41\n",
      "Accept                    â†’ Accept                   : 59\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-7): 0.61\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 39\n",
      "Accept                    â†’ Accept                   : 61\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-10): 0.60\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 40\n",
      "Accept                    â†’ Accept                   : 60\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-15): 0.64\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 36\n",
      "Accept                    â†’ Accept                   : 64\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-3): 0.53\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 47\n",
      "Accept                    â†’ Accept                   : 53\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-5): 0.51\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 49\n",
      "Accept                    â†’ Accept                   : 51\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-7): 0.54\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 46\n",
      "Accept                    â†’ Accept                   : 54\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-10): 0.57\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 43\n",
      "Accept                    â†’ Accept                   : 57\n",
      "\n",
      "âœ… Initial Results Accuracy (title+intro, top-15): 0.61\n",
      "\n",
      "ğŸ“Š Confusion Matrix (True â†’ Predicted):\n",
      "Accept                    â†’ Reject                   : 39\n",
      "Accept                    â†’ Accept                   : 61\n",
      "\n",
      "âœ… Best Configuration:\n",
      "Model: 1\n",
      "Top-k: 15\n",
      "Accuracy: 0.6400\n"
     ]
    }
   ],
   "source": [
    "# evaluate_accuracy_with_intro(n=len(titles), k=10)\n",
    "results = []\n",
    "k_values = [3, 5, 7, 10, 15]\n",
    "for i in range(3):\n",
    "    for k in k_values:\n",
    "        out = evaluate_accuracy_with_intro(n=len(titles), k=k,modelInd=i)\n",
    "        results.append((i, k, out[0]))\n",
    "        \n",
    "sorted_results = sorted(results, key=lambda x: (-x[2], x[1]))\n",
    "best = sorted_results[0]\n",
    "print(\"\\nâœ… Best Configuration:\")\n",
    "print(f\"Model: {best[0]}\")\n",
    "print(f\"Top-k: {best[1]}\")\n",
    "print(f\"Accuracy: {best[2]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
